{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9bvCq6rwsF18mJUGSeBSX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Festuskipkoech/Festus_data-science/blob/main/DecisionTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Notes\n",
        "\n",
        "## 1. What is a Decision Tree?\n",
        "A **decision tree** is a supervised machine learning algorithm used for classification and regression tasks. It models decisions and their possible consequences as a tree-like structure.\n",
        "\n",
        "- **Root Node**: Represents the entire dataset, which is split into subsets based on a feature.\n",
        "- **Leaf Node**: Represents the final output (class label or value).\n",
        "- **Branches**: Represent the decision rules based on feature values that lead to further splits.\n",
        "\n",
        "## 2. Structure of a Decision Tree\n",
        "- **Nodes**: Points in the tree where the dataset is split based on feature values.\n",
        "  - **Root Node**: The first node where the dataset is divided.\n",
        "  - **Internal Nodes**: Nodes between the root and leaf nodes.\n",
        "  - **Leaf Nodes**: Final decision or output.\n",
        "- **Edges/Branches**: Represent the conditions leading to the next node or final decision.\n",
        "\n",
        "## 3. Decision Tree Terminology\n",
        "- **Feature/Attribute**: A characteristic used to split the data (e.g., age, income, etc.).\n",
        "- **Split**: The process of dividing a dataset based on a featureâ€™s value.\n",
        "- **Impurity**: A measure of how mixed the data is in a node. The goal is to reduce impurity by splitting the data.\n",
        "  - Common impurity measures:\n",
        "    - **Gini Index**\n",
        "    - **Entropy**\n",
        "    - **Variance (for regression)**\n",
        "\n",
        "## 4. How Decision Trees Work\n",
        "1. **Select the Best Feature**: Choose a feature that best splits the data (using impurity measures like Gini Index or Entropy).\n",
        "2. **Split the Dataset**: Divide the data into subsets based on the selected feature.\n",
        "3. **Repeat**: Apply the same process recursively on the subsets (recursion continues until stopping conditions are met, such as pure nodes or depth limit).\n",
        "4. **Assign Class/Value**: Once leaf nodes are reached, assign a class (classification) or a value (regression).\n",
        "\n",
        "## 5. Impurity Measures\n",
        "- **Gini Index**:\n",
        "  - Measures the likelihood of misclassifying a randomly chosen element.\n",
        "  - Formula:  \n",
        "    \\( Gini(D) = 1 - \\sum_{i=1}^k p_i^2 \\), where \\( p_i \\) is the proportion of class \\( i \\) in the dataset.\n",
        "- **Entropy**:\n",
        "  - Measures the disorder or randomness in a dataset.\n",
        "  - Formula:  \n",
        "    \\( Entropy(D) = - \\sum_{i=1}^k p_i \\log_2(p_i) \\)\n",
        "- **Variance** (for Regression):\n",
        "  - Measures the variance within the dataset and attempts to minimize it by splitting the data.\n",
        "\n",
        "## 6. Splitting Criteria\n",
        "- **Best Split Selection**: Select the feature and threshold that results in the lowest impurity (i.e., the most \"pure\" node).\n",
        "  - **For classification**, aim to create homogenous classes.\n",
        "  - **For regression**, aim to create subsets where the variance is minimized.\n",
        "\n",
        "## 7. Overfitting and Pruning\n",
        "- **Overfitting**: Occurs when the tree model is too complex, capturing noise and small fluctuations in the training data, leading to poor generalization.\n",
        "  - **Signs of Overfitting**: Very deep trees, perfect training accuracy, but poor test accuracy.\n",
        "- **Pruning**: A technique to simplify the model by removing branches that have little importance.\n",
        "  - **Pre-Pruning**: Limit the tree depth, minimum number of samples per leaf, or other constraints to avoid overfitting.\n",
        "  - **Post-Pruning**: Build the tree fully, then trim branches that do not improve performance on a validation set.\n",
        "\n",
        "## 8. Advantages of Decision Trees\n",
        "- **Easy to understand** and interpret (visualizable).\n",
        "- **Non-linear**: Can model non-linear relationships.\n",
        "- **Can handle both numerical and categorical data**.\n",
        "- **No need for feature scaling**.\n",
        "- **Works well with missing data** (by treating missing values as a separate branch).\n",
        "\n",
        "## 9. Disadvantages of Decision Trees\n",
        "- **Overfitting**: If not pruned, decision trees can become overly complex and overfit to training data.\n",
        "- **Instability**: Small changes in the data can lead to very different tree structures.\n",
        "- **Biased towards features with more levels**: Features with more unique values can dominate splits.\n",
        "\n",
        "## 10. Common Decision Tree Algorithms\n",
        "- **ID3 (Iterative Dichotomiser 3)**:\n",
        "  - Uses **entropy** and **information gain** to split nodes.\n",
        "  - Works well for categorical data.\n",
        "- **C4.5**:\n",
        "  - Improved version of ID3 that uses **gain ratio** instead of information gain and handles both categorical and continuous attributes.\n",
        "- **CART (Classification and Regression Trees)**:\n",
        "  - Uses **Gini index** for classification and **variance reduction** for regression.\n",
        "\n",
        "## 11. Decision Tree for Regression\n",
        "- **Regression Trees** are used when the output is continuous (e.g., predicting house prices).\n",
        "- The tree splits the data at each node to minimize variance within the resulting groups.\n",
        "- At each leaf, the output value is typically the **mean** of the target values in that node.\n",
        "\n",
        "## 12. Hyperparameters of Decision Trees\n",
        "- **Max Depth**: Maximum depth of the tree.\n",
        "- **Min Samples Split**: Minimum number of samples required to split an internal node.\n",
        "- **Min Samples Leaf**: Minimum number of samples required to be at a leaf node.\n",
        "- **Max Features**: Maximum number of features to consider for each split.\n",
        "- **Criterion**: The function to measure the quality of a split (e.g., Gini or Entropy for classification, variance for regression).\n",
        "\n",
        "## 13. Use Cases\n",
        "- **Classification**: Spam detection, disease prediction, customer segmentation.\n",
        "- **Regression**: Price prediction, risk assessment, forecasting.\n"
      ],
      "metadata": {
        "id": "tZDrhaReDFbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees\n",
        "\n",
        "## Basic Concepts\n",
        "* Decision trees are hierarchical structures that make sequential decisions based on features\n",
        "* Each internal node represents a decision based on a feature\n",
        "* Each leaf node represents the final prediction/output\n",
        "* Can be used for both classification and regression problems\n",
        "\n",
        "## Key Components\n",
        "* Root Node: Top node, represents entire dataset\n",
        "* Internal/Decision Nodes: Test condition on a feature\n",
        "* Branches: Possible outcomes from a decision node\n",
        "* Leaf/Terminal Nodes: Final predictions\n",
        "* Path: Sequence from root to leaf node\n",
        "\n",
        "## Advantages\n",
        "* Easy to understand and interpret\n",
        "* Can handle both numerical and categorical data\n",
        "* Requires minimal data preprocessing\n",
        "* Can model non-linear relationships\n",
        "* Handles missing values well\n",
        "\n",
        "## Disadvantages\n",
        "* Can create overly complex trees (overfitting)\n",
        "* Can be unstable (small changes in data can create very different trees)\n",
        "* May create biased trees if classes are imbalanced\n",
        "* Single trees often have lower prediction accuracy\n",
        "\n",
        "## Important Parameters\n",
        "* max_depth: Maximum depth of tree\n",
        "* min_samples_split: Minimum samples needed to split node\n",
        "* min_samples_leaf: Minimum samples required at leaf node\n",
        "* max_features: Number of features to consider for best split\n",
        "* criterion: Measure for quality of split (gini/entropy for classification, mse/mae for regression)\n",
        "\n",
        "## Splitting Criteria\n",
        "### For Classification\n",
        "* Gini Impurity: Measures probability of incorrect classification\n",
        "* Entropy: Measures disorder/uncertainty in data\n",
        "* Information Gain: Reduction in entropy after split\n",
        "\n",
        "### For Regression\n",
        "* Mean Squared Error (MSE)\n",
        "* Mean Absolute Error (MAE)\n",
        "* Standard Deviation Reduction\n",
        "\n",
        "## Pruning Techniques\n",
        "* Pre-pruning: Stop growing tree using parameters\n",
        "* Post-pruning: Grow full tree then remove branches\n",
        "* Cost complexity pruning: Balance accuracy vs complexity\n",
        "\n",
        "## Best Practices\n",
        "* Start with simple tree before complex models\n",
        "* Use cross-validation to find optimal parameters\n",
        "* Consider ensemble methods (Random Forest, XGBoost)\n",
        "* Balance model complexity vs interpretability\n",
        "* Visualize tree to understand decisions\n",
        "\n",
        "## Implementation Example\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "# Create and train model\n",
        "dt = DecisionTreeClassifier(\n",
        "    max_depth=3,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    criterion='gini'\n",
        ")\n",
        "\n",
        "# Train\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt.predict(X_test)"
      ],
      "metadata": {
        "id": "nonNRZWgTYwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.metrics import accuracy_score\n",
        "import graphviz\n",
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Get the feature names\n",
        "iris_feature_names = iris.feature_names\n",
        "\n",
        "# create a decision tree classifier with maximum depth of 3\n",
        "clf = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the testing data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# calculate the accuracy score of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy\", accuracy)\n",
        "\n",
        "\n",
        "dot_data = export_graphviz(\n",
        "    clf,  # The trained decision tree classifier\n",
        "    out_file=None,  # Export to string instead of file\n",
        "    feature_names=iris_feature_names,  # List of feature names\n",
        "    class_names=iris.target_names,  # List of class names\n",
        "    filled=True,  # Color the nodes\n",
        "    rounded=True,  # Round the edges of the boxes\n",
        "    special_characters=True  # Allow special characters in labels\n",
        ")\n",
        "graph = graphviz.Source(dot_data)  # Create a Graphviz Source object\n",
        "graph.render(\"Iris decision tree\", view=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w2ZWu6EJkbVu",
        "outputId": "56cefd9a-972a-4f98-b8c7-ff6c8de77066"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.9555555555555556\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Iris decision tree.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}