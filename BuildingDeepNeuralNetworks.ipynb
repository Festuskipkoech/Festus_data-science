{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuO2MCOXioLMP9MNfbcXvw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Festuskipkoech/Festus_data-science/blob/main/BuildingDeepNeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Concepts of Building Deep Neural Networks\n",
        "\n",
        "## 1. **Introduction to Deep Neural Networks (DNNs)**\n",
        "- Deep Neural Networks (DNNs) are neural networks with multiple hidden layers.\n",
        "- They enable the learning of hierarchical representations by extracting increasingly complex features from data.\n",
        "- Commonly used in applications like image classification, speech recognition, and natural language processing.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Key Components of a DNN**\n",
        "### a. **Input Layer**\n",
        "- Accepts input data in a structured format.\n",
        "- Input shape must match the feature dimensions of the dataset.\n",
        "\n",
        "### b. **Hidden Layers**\n",
        "- Contain neurons that process and transform input features.\n",
        "- Each layer extracts higher-level abstractions from the previous layer.\n",
        "\n",
        "### c. **Output Layer**\n",
        "- Provides the final prediction or decision based on the learned features.\n",
        "- The number of neurons depends on the task (e.g., 1 for binary classification, N for N-class classification).\n",
        "\n",
        "### d. **Activation Functions**\n",
        "- Introduce non-linearity to allow the network to learn complex patterns.\n",
        "  - Common functions:\n",
        "    - **ReLU**: Rectified Linear Unit (\\(f(x) = \\max(0, x)\\))\n",
        "    - **Sigmoid**: Maps values to the range (0, 1), used in binary classification.\n",
        "    - **Softmax**: Converts raw outputs into probabilities for multi-class classification.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Building Blocks of DNNs**\n",
        "### a. **Weights and Biases**\n",
        "- **Weights**: Parameters connecting neurons, updated during training.\n",
        "- **Biases**: Additional parameters allowing the network to shift activation functions.\n",
        "\n",
        "### b. **Loss Function**\n",
        "- Measures the difference between predicted and true values.\n",
        "- Guides the optimization process.\n",
        "- Examples:\n",
        "  - Mean Squared Error (MSE) for regression tasks.\n",
        "  - Cross-Entropy Loss for classification tasks.\n",
        "\n",
        "### c. **Optimization Algorithm**\n",
        "- Updates weights to minimize the loss function.\n",
        "- Popular optimizers:\n",
        "  - **Stochastic Gradient Descent (SGD)**: Basic optimizer for minimizing the loss.\n",
        "  - **Adam**: Combines momentum and adaptive learning rates for efficient optimization.\n",
        "\n",
        "### d. **Learning Rate**\n",
        "- Determines the step size for weight updates.\n",
        "- Too high: May overshoot optimal values.\n",
        "- Too low: Slows down convergence.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Steps to Build a DNN**\n",
        "1. **Prepare Data**:\n",
        "   - Normalize or standardize input features.\n",
        "   - Split data into training, validation, and test sets.\n",
        "\n",
        "2. **Define the Model**:\n",
        "   - Use libraries like TensorFlow, Keras, or PyTorch.\n",
        "   - Example (Keras):\n",
        "     ```python\n",
        "     from tensorflow.keras import models, layers\n",
        "\n",
        "     model = models.Sequential()\n",
        "     model.add(layers.Dense(128, activation='relu', input_shape=(input_dim,)))\n",
        "     model.add(layers.Dense(64, activation='relu'))\n",
        "     model.add(layers.Dense(1, activation='sigmoid'))\n",
        "     ```\n",
        "\n",
        "3. **Compile the Model**:\n",
        "   - Specify the loss function, optimizer, and evaluation metrics.\n",
        "   ```python\n",
        "   model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "TOtD6zso_z6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36GbE1-1_pKQ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "# Sequential allows you to stack layers in a sequence, where the output of one layer becomes the input to the next.\n",
        "from keras.layers import Dense\n",
        "# It's used in hidden layers to learn features and in output layers for tasks like classification or regression\n",
        "\n",
        "# Define the architecture of the network\n",
        "model =Sequential()\n",
        "model.add(Dense(128, activation='relu', input_dim=100))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy =model.evaluate(x_test, y_test, batch_size=32)\n"
      ]
    }
  ]
}